{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7379779,"sourceType":"datasetVersion","datasetId":4288635}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:36:15.477300Z","iopub.execute_input":"2025-04-20T08:36:15.477881Z","iopub.status.idle":"2025-04-20T08:36:24.349654Z","shell.execute_reply.started":"2025-04-20T08:36:15.477853Z","shell.execute_reply":"2025-04-20T08:36:24.348542Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:36:24.351702Z","iopub.execute_input":"2025-04-20T08:36:24.352007Z","iopub.status.idle":"2025-04-20T08:36:26.020313Z","shell.execute_reply.started":"2025-04-20T08:36:24.351981Z","shell.execute_reply":"2025-04-20T08:36:26.019433Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import os, json, uuid, time\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:36:26.032861Z","iopub.execute_input":"2025-04-20T08:36:26.033124Z","iopub.status.idle":"2025-04-20T08:36:26.047011Z","shell.execute_reply.started":"2025-04-20T08:36:26.033105Z","shell.execute_reply":"2025-04-20T08:36:26.046181Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngcp_key = user_secrets.get_secret(\"gcp_credentials\")\nGOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:36:30.007125Z","iopub.execute_input":"2025-04-20T08:36:30.007439Z","iopub.status.idle":"2025-04-20T08:36:30.522143Z","shell.execute_reply.started":"2025-04-20T08:36:30.007417Z","shell.execute_reply":"2025-04-20T08:36:30.521399Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:35:00.511273Z","iopub.status.idle":"2025-04-20T08:35:00.512082Z","shell.execute_reply.started":"2025-04-20T08:35:00.511895Z","shell.execute_reply":"2025-04-20T08:35:00.511912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/tmp/gcp_key.json\", \"w\") as f:\n    f.write(gcp_key)\n\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/tmp/gcp_key.json\"\n\n# Set your GCP project and region\nPROJECT_ID = \"ai-detection-457406\"\nREGION = \"us-central1\"\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n\nfrom google.cloud import aiplatform\nfrom google.cloud import storage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:35:00.512976Z","iopub.status.idle":"2025-04-20T08:35:00.513298Z","shell.execute_reply.started":"2025-04-20T08:35:00.513130Z","shell.execute_reply":"2025-04-20T08:35:00.513144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/ai-vs-human-text/AI_Human.csv\")\n\n# Normalize column names\ndf.columns = [col.strip().lower() for col in df.columns]\n\n# Convert label to readable format\ndf[\"generated\"] = df[\"generated\"].apply(lambda x: \"ai\" if x == 1.0 else \"human\")\ndf[\"text\"] = df[\"text\"].astype(str).str.strip()\ndf = df[[\"text\", \"generated\"]]\n\n# Estimate max samples per class (≈ 35%)\nai_count = df[df[\"generated\"] == \"ai\"].shape[0]\nhuman_count = df[df[\"generated\"] == \"human\"].shape[0]\ntarget_total = int(0.35 * (ai_count + human_count))\ntarget_per_class = target_total // 2\n\nprint(f\"Target per class: {target_per_class} samples\")\n\n# Sample balanced set\ndf_sampled = (\n    df.groupby(\"generated\")\n    .apply(lambda x: x.sample(min(len(x), target_per_class), random_state=42))\n    .reset_index(drop=True)\n)\n\nprint(f\"✅ Final sample size: {len(df_sampled)} rows\")\ndf[\"generated\"].value_counts()\ndf_sampled[\"generated\"].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:35:06.743694Z","iopub.execute_input":"2025-04-20T08:35:06.744010Z","iopub.status.idle":"2025-04-20T08:35:37.289977Z","shell.execute_reply.started":"2025-04-20T08:35:06.743985Z","shell.execute_reply":"2025-04-20T08:35:37.289277Z"}},"outputs":[{"name":"stdout","text":"Target per class: 85266 samples\n✅ Final sample size: 170532 rows\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1022477164.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: x.sample(min(len(x), target_per_class), random_state=42))\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"generated\nai       85266\nhuman    85266\nName: count, dtype: int64"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df_sampled[\"generated\"].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:35:42.783965Z","iopub.execute_input":"2025-04-20T08:35:42.784296Z","iopub.status.idle":"2025-04-20T08:35:42.817003Z","shell.execute_reply.started":"2025-04-20T08:35:42.784271Z","shell.execute_reply":"2025-04-20T08:35:42.816148Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"array(['ai', 'human'], dtype=object)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Convert float to readable string labels\ndf_sampled[\"generated\"] = df_sampled[\"generated\"].apply(lambda x: \"ai\" if x == 1.0 else \"human\")\n\n# Keep only necessary columns\ndf_sampled = df_sampled[[\"text\", \"generated\"]]\n\n# Optional: balance classes\nmin_count = df_sampled[\"generated\"].value_counts().min()\ndf_sampled = df_sampled.groupby(\"generated\").apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n\ndf_sampled.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:35:56.943309Z","iopub.execute_input":"2025-04-20T08:35:56.943684Z","iopub.status.idle":"2025-04-20T08:35:57.140485Z","shell.execute_reply.started":"2025-04-20T08:35:56.943658Z","shell.execute_reply":"2025-04-20T08:35:57.139648Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3103721576.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sampled = df_sampled.groupby(\"generated\").apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                text generated\n0  Many people love libraries, and fa enjoy go sh...     human\n1  In recent years, there has been a growing move...     human\n2  Today, automobiles are responsible for 12 perc...     human\n3  **Introduction**\\n\\nThe Face on Mass is a prom...     human\n4  Video communication is a new technology that e...     human","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Many people love libraries, and fa enjoy go sh...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In recent years, there has been a growing move...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Today, automobiles are responsible for 12 perc...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>**Introduction**\\n\\nThe Face on Mass is a prom...</td>\n      <td>human</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Video communication is a new technology that e...</td>\n      <td>human</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from collections.abc import Iterable\nimport random\n\nimport random\n\nmax_epochs = 1\nmax_steps = 250_000\nmax_examples = max_steps // max_epochs\n\n# ✅ Convert df_sampled to formatted examples\nexamples = df_sampled.apply(\n    lambda row: {\n        \"input_text\": row[\"text\"],\n        \"output_text\": \"This text was generated by AI.\" if row[\"generated\"] == \"ai\" else \"This text was written by a human.\"\n    },\n    axis=1\n).tolist()\n\n\n# ✅ Shuffle + trim if needed\nrandom.shuffle(examples)\n\nif len(examples) > max_examples:\n    examples = examples[:max_examples]\n    print(f\"🔁 Downsampled to {len(examples)} examples to stay under {max_steps} steps\")\n\n# ✅ Update input_data\ninput_data = {\"examples\": examples}\n\n# If you are re-running this lab, add your model_id here.\nmodel_id = None\n\n# Or try and find a recent tuning job.\nif not model_id:\n  queued_model = None\n  # Newest models first.\n  for m in reversed(client.tunings.list()):\n    # Only look at newsgroup classification models.\n    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n      # If there is a completed model, use the first (newest) one.\n      if m.state.name == 'JOB_STATE_SUCCEEDED':\n        model_id = m.name\n        print('Found existing tuned model to reuse.')\n        break\n\n      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n        # If there's a model still queued, remember the most recent one.\n        queued_model = m.name\n  else:\n    if queued_model:\n      model_id = queued_model\n      print('Found queued model, still waiting.')\n\n\n# Upload the training data and queue the tuning job.\nif not model_id:\n    try:\n        print(\"🚀 Launching tuning job (non-blocking)...\")\n        tuning_op = client.tunings.tune(\n            base_model=\"models/gemini-1.5-flash-001-tuning\",\n            training_dataset=input_data,\n            config=types.CreateTuningJobConfig(\n                tuned_model_display_name=\"ai-detector-model\",\n                batch_size=4,\n                epoch_count=1,\n            ),\n        )\n        model_id = tuning_op.name\n        print(f\"✅ Job launched! Model ID: {model_id}\")\n    except Exception as e:\n        print(\"❌ Failed to launch tuning job:\")\n        print(e)\n\nprint(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:47:07.898051Z","iopub.execute_input":"2025-04-20T08:47:07.898715Z"}},"outputs":[{"name":"stdout","text":"🚀 Launching tuning job (non-blocking)...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"existing_jobs = client.tunings.list()\n\n# Find most recent\nfor job in reversed(existing_jobs):\n    if \"ai-detector-model\" in job.name:\n        print(f\"✅ Found job: {job.name} | Status: {job.state.name}\")\n        tuning_op = job\n        break\nelse:\n    print(\"❌ No recent tuning jobs found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:49:00.085230Z","iopub.execute_input":"2025-04-20T08:49:00.085970Z","iopub.status.idle":"2025-04-20T08:49:00.528304Z","shell.execute_reply.started":"2025-04-20T08:49:00.085936Z","shell.execute_reply":"2025-04-20T08:49:00.527252Z"}},"outputs":[{"name":"stdout","text":"❌ No recent tuning jobs found\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}